# Qianfan-VL: é¢†åŸŸå¢å¼ºé€šç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç³»åˆ—

<p align="center">
  <strong>é€šè¿‡æŒç»­é¢„è®­ç»ƒå®ç°é¢†åŸŸèƒ½åŠ›å¢å¼º | 3Båˆ°70Bå‚æ•°è§„æ¨¡ | æ–‡æ¡£ç†è§£ä¸OCRèƒ½åŠ›å¢å¼º | æ”¯æŒæ€è€ƒæ¨ç†èƒ½åŠ›</strong>
</p>

<div align="center">

ğŸ¤— **[Hugging Face æ¨¡å‹åº“](https://huggingface.co/baidu)** |
ğŸ¤– **[ModelScope æ¨¡å‹åº“](https://modelscope.cn/organization/baidu-qianfan)** |
ğŸ“š **[ä½¿ç”¨æ•™ç¨‹ Cookbook](https://github.com/baidubce/qianfan-models-cookbook)** |
ğŸ“– **[æŠ€æœ¯åšå®¢](https://baidubce.github.io/Qianfan-VL)** |
ğŸ“„ **æŠ€æœ¯æŠ¥å‘Š [å¾…æ›´æ–°]**

</div>

---

## æ¨¡å‹ä»‹ç»

Qianfan-VLæ¨¡å‹ç³»åˆ—æ˜¯åœ¨ä¼ä¸šçº§åº”ç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹çš„åœºæ™¯ä¸­è¿›è¡Œå¼ºåŒ–çš„é€šç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå…·å¤‡åŸºç¡€çš„é€šç”¨èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨äº§ä¸šè½åœ°çš„é«˜é¢‘åœºæ™¯æœ‰æ·±åº¦çš„ä¼˜åŒ–ã€‚é€šè¿‡ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½ï¼Œç²¾å‡†æ»¡è¶³ä¸åŒåœºæ™¯ä¸‹çš„å¤šæ¨¡æ€ç†è§£éœ€æ±‚ã€‚

## æ ¸å¿ƒç‰¹æ€§

### ğŸš€ å¤šå°ºå¯¸æ¨¡å‹
æä¾›3Bã€8Bã€70Bä¸‰ç§è§„æ ¼çš„æ¨¡å‹ï¼Œæ»¡è¶³ä»ç«¯ä¾§åˆ°äº‘ç«¯çš„ä¸åŒåœºæ™¯éœ€æ±‚

### ğŸ“ OCRä¸æ–‡æ¡£ç†è§£å¢å¼º
- **å…¨åœºæ™¯OCRè¯†åˆ«**ï¼šæ”¯æŒæ‰‹å†™ä½“ã€å°åˆ·ä½“ã€åœºæ™¯æ–‡å­—ã€å…¬å¼ç­‰å¤šç§æ–‡å­—è¯†åˆ«
- **å¤æ‚ç‰ˆé¢ç†è§£**ï¼šè¡¨æ ¼è§£æã€å›¾è¡¨ç†è§£ã€æ–‡æ¡£ç»“æ„åŒ–ç­‰èƒ½åŠ›
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šä¸­è‹±æ–‡åŠå¤šè¯­è¨€æ–‡æ¡£å¤„ç†èƒ½åŠ›

### ğŸ§  æ€è€ƒæ¨ç†èƒ½åŠ›
8Bå’Œ70Bæ¨¡å‹æ”¯æŒæ€è€ƒé“¾ï¼ˆChain-of-Thoughtï¼‰èƒ½åŠ›ï¼Œåœ¨æ•°å­¦ã€æ¨ç†è®¡ç®—ç­‰å¤æ‚åœºæ™¯å±•ç°å“è¶Šè¡¨ç°ï¼Œå¯åº”ç”¨äºè¾…åŠ©æ•™å­¦ã€æ‹ç…§è§£é¢˜ã€è‡ªåŠ¨åˆ¤é¢˜ç­‰åœºæ™¯

## æ¨¡å‹è§„æ ¼

| æ¨¡å‹åç§° | å‚æ•°é‡ | ä¸Šä¸‹æ–‡é•¿åº¦ | æ”¯æŒæ€è€ƒ | é€‚ç”¨åœºæ™¯ | æ¨¡å‹ä¸‹è½½ |
|---------|--------|-----------|---------|----------|---------|
| **Qianfan-VL-3B** | 3B | 32k | âŒ | ç«¯ä¸Šå®æ—¶åœºæ™¯ã€OCRæ–‡å­—è¯†åˆ« | ğŸ¤— **[HuggingFace](https://huggingface.co/baidu/Qianfan-VL-3B)** / ğŸ¤– **[ModelScope](https://modelscope.cn/models/baidu-qianfan/Qianfan-VL-3B)** |
| **Qianfan-VL-8B** | 8B | 32k | âœ… | æœåŠ¡ç«¯é€šç”¨åœºæ™¯ã€å¾®è°ƒä¼˜åŒ–åœºæ™¯ | ğŸ¤— **[HuggingFace](https://huggingface.co/baidu/Qianfan-VL-8B)** / ğŸ¤– **[ModelScope](https://modelscope.cn/models/baidu-qianfan/Qianfan-VL-8B)** |
| **Qianfan-VL-70B** | 70B | 32k | âœ… | ç¦»çº¿æ•°æ®åˆæˆã€å¤æ‚æ¨ç†è®¡ç®—åœºæ™¯ | ğŸ¤— **[HuggingFace](https://huggingface.co/baidu/Qianfan-VL-70B)** / ğŸ¤– **[ModelScope](https://modelscope.cn/models/baidu-qianfan/Qianfan-VL-70B)** |

## æŠ€æœ¯ä¼˜åŠ¿

### ğŸš€ å¤šé˜¶æ®µé¢†åŸŸå¢å¼ºæŒç»­é¢„è®­ç»ƒæŠ€æœ¯
é‡‡ç”¨åˆ›æ–°çš„å››é˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œä»è·¨æ¨¡æ€å¯¹é½åˆ°é€šç”¨çŸ¥è¯†æ³¨å…¥ï¼Œå†åˆ°é¢†åŸŸå¢å¼ºçŸ¥è¯†æ³¨å…¥å’Œåè®­ç»ƒå¯¹é½ï¼Œåœ¨ä¿æŒé€šç”¨èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡é¢†åŸŸä¸“é¡¹èƒ½åŠ›ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®é…æ¯”å’Œè®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†é€šç”¨ä¸ä¸“ä¸šèƒ½åŠ›çš„è‰¯å¥½å¹³è¡¡ã€‚

### ğŸ¯ é«˜ç²¾åº¦æ•°æ®åˆæˆ
æ„å»ºäº†è¦†ç›–æ–‡æ¡£è¯†åˆ«ã€æ•°å­¦è§£é¢˜ã€å›¾è¡¨ç†è§£ã€è¡¨æ ¼è¯†åˆ«ã€å…¬å¼è¯†åˆ«ã€è‡ªç„¶åœºæ™¯OCRç­‰æ ¸å¿ƒä»»åŠ¡çš„å¤šä»»åŠ¡æ•°æ®åˆæˆç®¡çº¿ã€‚ç»“åˆä¼ ç»ŸCVæ¨¡å‹å’Œç¨‹åºåŒ–ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡ç²¾ç»†åŒ–çš„ç®¡çº¿è®¾è®¡å’Œä¸­é—´è¿‡ç¨‹æ•°æ®æ„é€ ï¼Œå®ç°äº†é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„é«˜æ•ˆç”Ÿäº§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é•¿å°¾åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚

### âš¡ å¤§è§„æ¨¡æ˜†ä»‘èŠ¯é›†ç¾¤å¹¶è¡Œè®­ç»ƒ
åŸºäºç™¾åº¦è‡ªç ”æ˜†ä»‘èŠ¯P800èŠ¯ç‰‡ï¼Œé€šè¿‡5000+å¡çš„è¶…å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿå®Œæˆå…¨éƒ¨æ¨¡å‹è§„æ¨¡çš„è®­ç»ƒã€‚é‡‡ç”¨3Då¹¶è¡Œè®­ç»ƒç­–ç•¥å’Œé€šä¿¡-è®¡ç®—èåˆæŠ€æœ¯ï¼Œå®ç°äº†90%+çš„é›†ç¾¤æ‰©å±•æ•ˆç‡ï¼Œ3T tokensè®­ç»ƒæ•°æ®çš„é«˜æ•ˆå¤„ç†ï¼Œå±•ç¤ºäº†å›½äº§AIåŸºç¡€è®¾æ–½çš„æˆç†Ÿèƒ½åŠ›ã€‚

## æ€§èƒ½æŒ‡æ ‡

### é€šç”¨èƒ½åŠ›è¯„æµ‹

*æ³¨ï¼šåŠ ç²—æ•°å€¼è¡¨ç¤ºè¯¥æŒ‡æ ‡åœ¨æ‰€æœ‰æ¨¡å‹ä¸­æ’åå‰ä¸¤ä½*

| åŸºå‡†æµ‹è¯• | Qianfan-VL-3B | Qianfan-VL-8B | Qianfan-VL-70B | Intern3-VL-8B | Intern3-VL-78B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|---------|---------------|---------------|----------------|---------------|----------------|---------------|----------------|
| **A-Bench_VAL** | 75.65 | 75.72 | **78.1** | 75.86 | 75.86 | 76.49 | **79.22** |
| **CCBench** | 66.86 | 70.39 | **80.98** | **77.84** | 70.78 | 57.65 | 73.73 |
| **SEEDBench_IMG** | 76.55 | 78.02 | **79.13** | 77.00 | 77.52 | 76.98 | **78.34** |
| **SEEDBench2_Plus** | 67.59 | 70.97 | **73.17** | 69.52 | 68.47 | 70.93 | **73.25** |
| **ScienceQA_TEST** | 95.19 | **97.62** | **98.76** | 97.97 | 97.17 | 85.47 | 92.51 |
| **ScienceQA_VAL** | 93.85 | **97.62** | **98.81** | 97.81 | 95.14 | 83.59 | 91.32 |
| **MMT-Bench_VAL** | 62.23 | 63.22 | **71.06** | 65.17 | 63.67 | 61.40 | **69.49** |
| **MTVQA_TEST** | 26.5 | 30.14 | **32.18** | 30.30 | 27.62 | 29.08 | **31.48** |
| **BLINK** | 49.97 | 56.81 | **59.44** | 55.87 | 51.87 | 54.55 | **63.02** |
| **MMStar** | 57.93 | 64.07 | **69.47** | **68.40** | 66.07 | 61.53 | 66.00 |
| **POPE** | 85.08 | 86.06 | 88.97 | **90.59** | 88.87 | 85.97 | 83.35 |
| **RefCOCO (Avg)** | 85.94 | 89.37 | **91.01** | 89.65 | **91.40** | 86.56 | 90.25 |

### OCRä¸æ–‡æ¡£ç†è§£èƒ½åŠ›

| åŸºå‡†æµ‹è¯• | Qianfan-VL-3B | Qianfan-VL-8B | Qianfan-VL-70B | Qwen2.5-VL-3B | Intern3-VL-8B | Intern3-VL-78B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|---------|---------------|---------------|----------------|---------------|---------------|----------------|---------------|----------------|
| **OCRBench** | 831 | 854 | 873 | 810 | **881** | 847 | **883** | 874 |
| **AI2D_TEST** | 81.38 | **85.07** | **87.73** | 77.07 | **85.07** | 83.55 | 80.472 | 83.84 |
| **OCRVQA_TEST** | **66.15** | **68.98** | **74.06** | 69.24 | 39.03 | 35.58 | **71.02** | 66.8 |
| **TextVQA_VAL** | 80.11 | 82.13 | **84.48** | 79.09 | 82.15 | 83.52 | **84.962** | 83.26 |
| **DocVQA_VAL** | 90.85 | 93.54 | 94.75 | 92.71 | 92.04 | 83.82 | **94.91** | **95.75** |
| **ChartQA_TEST** | 81.79 | **87.72** | **89.6** | 83.4 | 85.76 | 82.04 | 86.68 | 87.16 |

### æ•°å­¦æ¨ç†èƒ½åŠ›

| åŸºå‡†æµ‹è¯• | Qianfan-VL-8B | Qianfan-VL-70B | Intern3-VL-8B | Intern3-VL-78B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|---------|---------------|----------------|---------------|----------------|---------------|----------------|
| **Mathvista-mini** | **69.19** | **78.6** | 69.5 | 71.1 | 69.5 | 70.1 |
| **Mathvision** | **32.82** | **50.29** | 21.48 | 33.48 | 29.61 | 34.8 |
| **Mathverse** | **48.4** | **61.04** | 30.96 | 43.32 | 43.68 | 49.26 |
| **ChartQA Pro** | **50.41** | **52** | 19.38 | 47.92 | 37.32 | 44.43 |
| **HallusionBench** | **51.72** | **54.52** | 49.7 | 40.5 | 49.2 | 40.2 |
| **InHouse Dataset A** | **59.87** | **71.78** | 26 | 43.40 | 40.64 | 41.47 |
| **InHouse Dataset B** | **61.33** | **75.6** | 26.81 | 39.7 | 36.25 | 42.65 |

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install transformers torch torchvision pillow
```

### ä½¿ç”¨ Transformers

```python
import torch
import torchvision.transforms as T
from torchvision.transforms.functional import InterpolationMode
from transformers import AutoModel, AutoTokenizer
from PIL import Image

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    # è®¡ç®—ç°æœ‰å›¾åƒçš„å®½é«˜æ¯”
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

    # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡çš„å®½é«˜æ¯”
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    # è®¡ç®—ç›®æ ‡å®½åº¦å’Œé«˜åº¦
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

    # è°ƒæ•´å›¾åƒå¤§å°
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        # åˆ†å‰²å›¾åƒ
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

def load_image(image_file, input_size=448, max_num=12):
    image = Image.open(image_file).convert('RGB')
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(image) for image in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values

# åŠ è½½æ¨¡å‹
MODEL_PATH = "Baidu/Qianfan-VL-8B"  # æˆ–é€‰æ‹© Qianfan-VL-3B, Qianfan-VL-70B
model = AutoModel.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto"
).eval()
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

# åŠ è½½å¹¶å¤„ç†å›¾åƒ
pixel_values = load_image("./example/scene_ocr.png").to(torch.bfloat16)

# æ¨ç†
prompt = "<image>è¯·è¯†åˆ«å›¾ä¸­æ‰€æœ‰æ–‡å­—"
with torch.no_grad():
    response = model.chat(
        tokenizer,
        pixel_values=pixel_values,
        question=prompt,
        generation_config={"max_new_tokens": 512},
        verbose=False
    )
print(response)
```

### ä½¿ç”¨ vLLM

æ‚¨å¯ä»¥ä½¿ç”¨ vLLM çš„å®˜æ–¹ Docker é•œåƒéƒ¨ç½² Qianfan-VLï¼Œå®ç°é«˜æ€§èƒ½æ¨ç†å’Œ OpenAI å…¼å®¹çš„ APIï¼š

#### å¯åŠ¨ vLLM æœåŠ¡

```bash
docker run -d --name qianfan-vl \
  --gpus all \
  -v /path/to/Qianfan-VL-8B:/model \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model /model \
  --served-model-name qianfan-vl \
  --trust-remote-code \
  --hf-overrides '{"architectures":["InternVLChatModel"],"model_type":"internvl_chat"}'
```

#### è°ƒç”¨ API

```bash
curl 'http://127.0.0.1:8000/v1/chat/completions' \
  --header 'Content-Type: application/json' \
  --data '{
    "model": "qianfan-vl",
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "image_url",
            "image_url": {
              "url": "https://example.com/image.jpg"
            }
          },
          {
            "type": "text",
            "text": "<image>è¯·è¯†åˆ«å›¾ä¸­æ‰€æœ‰æ–‡å­—"
          }
        ]
      }
    ]
  }'
```

æ›´å¤šä½¿ç”¨ç¤ºä¾‹è¯·å‚è€ƒ [Cookbook](https://github.com/baidubce/qianfan-models-cookbook/blob/main/qianfan-vl/qianfan_vl_example.ipynb)

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†Qianfan-VLï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{qianfan-vl-2025,
  title={Qianfan-VL: Domain-Enhanced General Vision-Language Model Series},
  author={Baidu Qianfan Team},
  year={2025},
  publisher={Baidu AI Cloud},
  howpublished={\url{https://github.com/baidubce/Qianfan-VL}}
}
```

## è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª Apache 2.0 è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## è”ç³»æˆ‘ä»¬

- å®˜æ–¹ç½‘ç«™: [ç™¾åº¦æ™ºèƒ½äº‘åƒå¸†](https://qianfan.cloud.baidu.com)
- æŠ€æœ¯æ”¯æŒ: qianfan-support@baidu.com
- GitHub Issues: [æäº¤é—®é¢˜](https://github.com/baidubce/Qianfan-VL/issues)

---

<p align="center">
  <strong>ç™¾åº¦æ™ºèƒ½äº‘åƒå¸†å¤§æ¨¡å‹å¹³å° | 2025</strong>
</p>